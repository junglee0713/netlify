---
title: "Lecture 11"
author: "Jung-Jin Lee"
date: "Apr 16, 2019"
output:
  xaringan::moon_reader:
    css: ["default", "default-fonts", "Drexel.css"] 
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
 
```{r, message = F, echo = F, warning = F}
library(tidyverse) 
library(gridExtra)
library(knitr) 
library(latex2exp)
opts_chunk$set(
  tidy = FALSE,
  cache = FALSE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  dpi = 300,
  fig.align = "center",
  fig.width = 4,
  fig.height = 4
  )
```

## Multiple linear regression, review

```{r}
donkey <- read.table("donkey.tsv", header = T, sep = "\t") 
dim(donkey)
head(donkey)
```

---

## Multiple linear regression, review

```{r}
fit <- lm(Bodywt ~ Length + Heartgirth + Height, data = donkey)
summary(fit)$coefficients
```

--

Based on the output, we conclude that 

\begin{align}
Bodywt = &-217.706 \\
  &+0.916 \times Length    \\
  &+2.124 \times Heartgirth    \\
  &+0.261 \times Height  
\end{align}

---

## Multiple linear regression, review

- Interpretation: 0.916 represents the change in Bodywt for a unit change in Length, when **the other explanatory variables, Heartgirth and Height, are held constant** (i.e. after controlling for these variables)

--

- Similarly, when **Length and Height** are fixed, a unit increase in `Heartgirth` will result in increase of `Bodywt` by 2.124.

--

- Prediction: a donkey with `Length` 85cm, `Heartgirth` 110cm, and `Height` 96cm will weigh approximately 
$$ -217.706 + 0.916 \times 85 + 2.124 \times 110 + 0.261 \times 96 = 118.85 \text{ kg}.$$ 

---

## Multiple linear regression, review

- RSS (residual sum of squares): sum of the squares of residuals:
$$RSS=\sum_i r_i^2 = \sum_i\left[y_i - \text{fitted value}\right]^2$$

```{r}
rss <- sum(fit$residuals^2)
rss
```

```{r}
## Or alternatively
rss2 <- sum((donkey$Bodywt-fit$fitted.values)^2)
rss2
```

---

## Multiple linear regression, review

- SSreg (regression sum of squares): sum of squares of the difference between fitted value and $\bar{y}$:
$$SSreg = \sum_i \left[\text{fitted value}-\bar{y}\right]^2$$

```{r}
ssreg <- sum((fit$fitted.values-mean(donkey$Bodywt))^2)
ssreg
```

---

## Multiple linear regression, review

- TSS (total sum of squares): sum of the squares of the difference between each $y_i$ value and the mean $\bar{y}$:
$$TSS=\sum_i\left(y_i-\bar{y}\right)^2$$

```{r}
tss <- sum((donkey$Bodywt-mean(donkey$Bodywt))^2)
tss
```

```{r}
## It is known that TSS = RSS + SSreg
rss + ssreg
```

---

## Multiple linear regression, review

- $R^2$ (R-squared or coefficient of determination): measure of goodness of a model
$$R^2 = \frac{SSreg}{TSS} = 1 - \frac{RSS}{TSS}$$
```{r}
r2 <- ssreg/tss
r2
```

```{r}
summary(fit)$r.squared # r.squared is from summary(fit), not fit
```

---

## Model comparison

- Consider a model
$$\textbf{Model 1: } \texttt{Bodywt}=\beta_0+\beta_1 \texttt{Length}.$$
How good is this model?

--

- One approach to assess a model is to compute $R^2$.
  - A good model will yield an $R^2$ close to 1. 

```{r}
fit1 <- lm(Bodywt ~ Length, data = donkey) ## Model 1
r.squared1 <- summary(fit1)$r.squared
r.squared1
```

---

## Model comparison

- Consider another model
$$\textbf{Model 2: } \texttt{Bodywt}=\beta_0+\beta_1 \texttt{Height}.$$

```{r}
fit2 <- lm(Bodywt ~ Height, data = donkey) ## Model 2
r.squared2 <- summary(fit2)$r.squared
r.squared2
```

- **Model 1** has bigger $R^2$ than **Model 2**, so we can conclude that **Model 1** is a better model than **Model 2**.

---

## Model comparison

.pull-left[
```{r, echo = F}
ggplot(donkey, aes(Length, Bodywt)) + 
  geom_point() + 
  geom_abline(intercept = fit1$coefficients[1], slope = fit1$coefficients[2], color = "red")
```
]

.pull-right[
```{r, echo = F}
ggplot(donkey, aes(Height, Bodywt)) + 
  geom_point() + 
  geom_abline(intercept = fit2$coefficients[1], slope = fit2$coefficients[2], color = "blue")
```
]

---

## Model comparison

- Now consider the model
$$\textbf{Model 3: } \texttt{Bodywt}=\beta_0+\beta_1\texttt{Length}+\beta_2\texttt{Height}.$$
How good is this model?

--

```{r}
fit3 <- lm(Bodywt ~ Length + Height, data = donkey) ## Model 3
r.squared3 <- summary(fit3)$r.squared
r.squared3
```

- This has the biggest $R^2$ so far. Is this surprising?

--

- **Model 1** and **Model 2** are _nested_ within **Model 3**, so $R^2$ being the bigger in **Model 3** than **Model 1** or **Model 2** is expected. 

--

- In general, the more predictors are added, the bigger $R^2$ is. 

---

## Model comparison

- Now consider yet another model

\begin{align}
\textbf{Model 4: } \texttt{Bodywt} = & \beta_0 +  \beta_1\texttt{Length} + \beta_2\texttt{Height} \\
  & +\beta_3\texttt{Heartgirth} + \beta_4\texttt{Umbgirth}
\end{align}

```{r}
fit4 <- lm(Bodywt ~ Length + Height + Heartgirth + Umbgirth, data = donkey) ## Model 4
r.squared4 <- summary(fit4)$r.squared
r.squared4
```

--

- Although this model the biggest $R^2$ so far, it is more complex than other models. 

--

- Need to consider trade-off between simple model (less number of variables) vs good fit (higher $R^2$)

--

- Idea: if a bigger model does not considerably increase $R^2$, then stick to simpler model. 

---

## Comparison of nested models

- Which is better? $\textbf{Model 3}$ or $\textbf{Model 4}$?

$$\textbf{Model 3: } \texttt{Bodywt}=\beta_0+\beta_1\texttt{Length}+\beta_2\texttt{Height}$$

\begin{align}
\textbf{Model 4: } \texttt{Bodywt} = & \beta_0 +  \beta_1\texttt{Length} + \beta_2\texttt{Height} \\
  & +\beta_3\texttt{Heartgirth} + \beta_4\texttt{Umbgirth}
\end{align}

--

- Recall that $\textbf{Model 4}$ had a bigger $R^2$ than $\textbf{Model 3}$:

```{r}
r.squared3
r.squared4
```

--

- However, $\textbf{Model 4}$ is more complicated. 

---

## Comparison of nested models

- Idea: if both estimated $\beta_3$ and $\beta_4$ turn out to be close to $0$, then it would mean that `Heartgirth`

--

- Fact: Under the assumption that both $\beta_3=\beta_4=0$,

  - It is known that 
$$F = \frac{(RSS_{M3}-RSS_{M4})/2}{RSS_{M4}/(386-5)}$$
follows an $F$-distribution with $df1=2$ and $df2=386-5=381$ degrees of freedom

  - Here $RSS_{M3}$ (respectively, $RSS_{M4}$) is the RSS from $\textbf{Model 3}$ (respectively, from $\textbf{Model 4}$)
  
  - 2 comes from how many more $\beta$s $\textbf{Model 4}$ has compared to $\textbf{Model 3}$, 386 from sample size, 5 from the number of $\beta$s in $\textbf{Model 4}$.
  
---

## Comparison of nested models

```{r}
rss3 <- sum(fit3$residuals^2)
rss4 <- sum(fit4$residuals^2)
f <- ((rss3-rss4)/2)/(rss4/(386-5))
print(rss3)
print(rss4)
print(f)
```

---

## Comparison of nested models

```{r, echo = F, fig.height = 3, fig.width = 4, out.width = "70%"}
f_2_23 = function(x) df(x, df1 = 2, df = 5)
gf <- ggplot(data.frame(x = 1), aes(x)) +
  scale_x_continuous(lim = c(0, 8), breaks = c(0, 6), labels = c("0", round(f, 3))) +
  theme_bw() + 
  theme_classic() +
  stat_function(fun = f_2_23, xlim = c(6, 8), geom = "area", fill = "red") + 
  stat_function(fun = f_2_23) + 
  ylim(0, 1) +
  ggtitle(TeX("F_{df1=2, df2=381}"))
print(gf)
```

```{r}
1-pf(f, df1 = 2, df2 = 381)
```

---

## Comparison of nested models

- With this extremely small $p$-value, we reject the null hypothesis that $\beta_3=\beta_4=0$. 

--

- This leads to either $\beta_3\neq 0$ or $\beta_4\neq 0$.

--

- So either `Heartgirth` or `Umbgirth` (or both) significantly affects `Bodywt`

--

- Final conclusion: $\textbf{Model 4}$ is preferred to $\textbf{Model 3}$.

---

## Comparison of nested models

```{r}
anova(fit3, fit4)
```

---

## Comparison of nested models: exercise

Suppose that there is reason to believe that powers of `Length` have something to do with `Bodywt`: 

$$\textbf{Model 5: } \texttt{Bodywt}=\beta_0+\beta_1\texttt{Length}+\beta_2\texttt{Length}^2,$$
$$\textbf{Model 6: } \texttt{Bodywt}=\beta_0+\beta_1\texttt{Length}+\beta_2\texttt{Length}^2+\beta_3\texttt{Length}^3,$$
$$\textbf{Model 7: } \texttt{Bodywt}=\beta_0+\beta_1\texttt{Length}+\beta_2\texttt{Length}^2+\beta_3\texttt{Length}^3+\beta_4\texttt{Length}^4.$$

---

## Comparison of nested models: exercise

```{r}
donkey <- donkey %>%
  mutate(Length2 = Length^2) %>%
  mutate(Length3 = Length^3) %>%
  mutate(Length4 = Length^4)

head(donkey)
```

---

## Comparison of nested models: exercise

```{r}
fit5 <- lm(Bodywt ~ Length + Length2, data = donkey) ## Model 5
fit6 <- lm(Bodywt ~ Length + Length2 + Length3, 
           data = donkey) ## Model 6
fit7 <- lm(Bodywt ~ Length + Length2 + Length3 + Length4,
           data = donkey) ## Model 7
```

Note that $\textbf{Model 5}$ is nested within $\textbf{Model 6}$, which is in turn nested within $\textbf{Model 7}$.

---

## Comparison of nested models: exercise

- Compare $\textbf{Model 6}$ and $\textbf{Model 7}$.

```{r}
anova(fit6, fit7)
```

Small $p$-value indicates that `Length4` significantly improves model, so $\textbf{Model 7}$ is preferred to $\textbf{Model 6}$.

---

## Comparison of nested models: exercise

- Compare $\textbf{Model 5}$ and $\textbf{Model 7}$.

```{r}
anova(fit5, fit7)
```

Small $p$-value indicates that having both `Length3` and `Length4` significantly improves model, so $\textbf{Model 7}$ is preferred to $\textbf{Model 5}$.

---

## Comparison of nested models: exercise

- Compare $\textbf{Model 5}$ and $\textbf{Model 6}$.

```{r}
anova(fit5, fit6)
```

$p$-value of 0.2284 indicates that additionally having `Length3` does not significantly improve model, so $\textbf{Model 5}$ is preferred to $\textbf{Model 6}$.

---

## lm(), revisited

Recall $\textbf{Model 4}$:

\begin{align}
\textbf{Model 4: } \texttt{Bodywt} = & \beta_0 +  \beta_1\texttt{Length} + \beta_2\texttt{Height} \\
  & +\beta_3\texttt{Heartgirth} + \beta_4\texttt{Umbgirth}
\end{align}

```{r}
summary(fit4)$coefficients
```

What is the meaning of the $p$-value $2.22\times 10^{-13}$ for `Length`?

---

## lm(), revisited

```{r}
fit_no_Length <- lm(Bodywt ~ Height + Heartgirth + Umbgirth, 
                    data = donkey) ## model without Length
anova(fit_no_Length, fit4)
```

--

- The meaning of the $p$-value $2.22\times 10^{-13}$: does adding `Length` to  
$$\texttt{Bodywt} = \beta_0 + \beta_1\texttt{Height} + \beta_2\texttt{Heartgirth} +\beta_3 \texttt{Umbgirth} $$
improve the model?

--

- Small $p$-value indicates that `Length` does improve the model.

---

## Comparison of non-nested models

- Compare $\textbf{Model 1}$ and $\textbf{Model 2}$:

$$\textbf{Model 1: } \texttt{Bodywt}=\beta_0+\beta_1 \texttt{Length}.$$

$$\textbf{Model 2: } \texttt{Bodywt}=\beta_0+\beta_1 \texttt{Height}.$$

```{r}
anova(fit1, fit2) ## This is not so right
```

This does not make sense because the models are not nested. 

---

## Comparison of non-nested models

- AIC (Akaike information criterion) deals with the trade-off between the goodness-of-fit and the complexity of a model.

--

- AIC can be used to compare models that are not nested within each other

--

- A model with smaller AIC score is better.

```{r}
AIC(fit1, fit2)
```

$\textbf{Model 1}$ is preferred. 

---

## Comparison of non-nested models

- AIC can be used to compare nested models, too

```{r}
AIC(fit5, fit6, fit7)
```

--

- BIC (Bayesian information criterion) is also used to compare non-nested models. 

--

- A model with smaller BIC score is better.

```{r}
BIC(fit5, fit6, fit7)
```

---

## Automatic Model Selection

There are several ways to find the best model: 

- Forward selection: start with the single explanatory variable that contributes the most to the explained variation in $y$, and include more variables in the modelling equation, progressively, until the addition of an extra variable does not significantly improve the situation. 

--

- Backward elimination: start with all the variables, and take them away sequentially, starting with the variable that contributes the least, until the deletion of a variable does not significantly reduce the amount of explained variation in $y$. 

--

- Both

--

- `step()` in R can be used to automatically find the best model. 

---

## Automatic Model Selection

```{css, echo = FALSE}
.tiny .remark-code { /*Change made here*/
  font-size: 90% !important;
}
```

```{r, eval = F}
fit_full <- lm(Bodywt ~ Age + Length + Length2 + Height + 
                 Heartgirth + Umbgirth, data = donkey)
step(fit_full)
```

.tiny[
```{r, echo = F}
step(fit_full)
```
]
